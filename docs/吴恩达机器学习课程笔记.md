# 吴恩达机器学习课程笔记

在两个月左右的学习之后，终于完成了吴恩达老师的机器学习课程。在此汇总学习笔记，既是自己复习，也是为准备学习这门课程的同学做一下简单的介绍。



## 课程大纲

![课程大纲](https://user-images.githubusercontent.com/9285301/62422760-215f3180-b6ea-11e9-919f-9e0659ff8589.jpg)

以上我总结的机器学习大纲，在接下来的文章中，我会汇总学习笔记中的一些关键内容，可能很杂，因为不像教材中循序渐进的提出问题、解决问题、优化算法，我只是将核心公式记录下来以便之后翻阅。



## 监督学习

训练集：

训练集中训练样本的数量用小写m表示

输入变量（往往被称为特征量）用小写x表示

输出变量（或称目标变量、预测结果）用小写y表示

使用(x, y)表示一个训练集

(x(i), y(i))表示训练集中的第i行

### 线性回归

学习算法通过学习训练集输出一个函数h(hypothesis，代表假设)

h写作
$$
hθ(x) = \theta_0 + \theta_1 x
$$
简写为h(x)

 

这样的模型被称为线性回归（Linear regression）模型

单个变量的线性回归，称为一元线性回归（Univariate LinearRegression）

 

当目标变量y的取值是连续的，这样的问题被称为回归问题。

当y只能取少量离散值时，问题被称为归类问题。

 

代价函数（又称损失函数、成本函数）：最常用的是平方损失函数

① 0-1损失函数    ② 平方损失函数    ③ 绝对值损失函数    ④对数损失函数    ⑤指数损失函数

 

平方损失函数：
$$
J(\theta_0 - \theta_1) = \frac{1}{2m} \sum\limits_{i=1}^m(\overline{y} - y)^2 = \frac{1}{2m} \sum\limits_{i=1}^m(h\theta(x_i) - y_i)^2
$$


#### 梯度下降算法：

梯度下降是迭代法的一种，可用于求解最小二乘问题（线性与非线性均可），是一种最优化算法，常用于机器学习和人工智能中用来递归地逼近最小偏差模型。基于基本的梯度下降方法发展了两种梯度下降方法，分别是随机梯度下降法和批量梯度下降法。

 

微分（导数）：
$$
\frac{∂}{∂x} x^2 y^2=2xy^2
$$

$$
\frac{∂}{∂y} x^2 y^2=2x^2 y
$$

$$
\frac{∂}{σx} x^2=2x
$$

梯度：对于可微的数量场f(x, y, z)，以
$$
(\frac{∂f}{∂x},\frac{∂f}{∂y},\frac{∂f}{∂z})
$$
为分量的向量场称为f的梯度或斜量。实际上就是分别对每个变量进行微分，然后用逗号隔开。

 

注：1.在单变量函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率。

2.在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。

 

梯度下降算法：
$$
\Theta = \Theta - \alpha\nabla J(\Theta)
$$

$$
此公式意义：J是关于Θ的一个函数，我们当前所处的位置为Θ^0 点，要从这个点走到J的最小值点。
$$

$$
首先我们确定前进的方向是梯度的反向，然后走一段距离的
步长为α，走完这个步长达到Θ^1 点。
$$

1. 多元线性回归

   训练集中有多种特征值的线性回归

   设：
   $$
   m为训练集中训练样本的数目
   $$

   $$
   n为训练集中特征值的数目
   $$

   $$
   x^{(i)}为第i个训练样本的特征值集合
   $$

   $$
   x^{(i)}_j为第i个训练样本的第j个特征值
   $$

   

   

   多元线性回归的假设(hypothesis)函数如下：
   $$
   h_θ(x)=θ_0+θ_1 x_1+θ_2 x_2+θ_3 x_3+…+ θ_n  x_n
   $$
   将上式转化为矩阵形式，我们设，以使θ和x的个数相等，上式可写作：
   $$
   h_\theta (x) = [\begin{array}{lcr}θ_0&θ_1&θ_2&…&θ_n\end{array}] * \left[\begin{array}{lcr}x_0 \\ x_1 \\ x_2 \\ …\\ x_n \end{array}\right] = \theta^T * x
   $$
   多元梯度下降：

$$
损失函数  J( θ_0, θ_1, …, θ_n  )=\frac{1}{2m} \sum\limits_{i=1}^m  (h_θ (x^{(i)})−y^{(i)})^2
$$

$$
   梯度下降迭代式： θ_j≔ θ_j − α\frac{∂}{∂θ_j}  J(θ_0, θ_1, …, θ_n )
$$

$$
   \frac{∂}{∂_j}J(θ_0, θ_1, …, θ_n ) \\ =\frac{∂}{∂_j}\frac{1}{2m}\sum\limits_{i=1}^m  (h_θ (x^{(i)})−y^{(i)})^2 \\ = 2∗\frac{1}{2m}(h_θ(x^{(i)}) − y^{(i)})x^{(i)}
   \\ =\frac{1}{m}(h_θ  (x^{(i)}) − y^{(i)})x^{(i)}
$$

由上述求微分过程可得梯度下降迭代式：
$$
   θ_j≔ θ_j  −α \frac{1}{m} \sum\limits_{i=1}^m  (h_θ (x^{(i)}) − y^{(i)})  x_j^{(i)}    \\ for j≔0… n
$$
   复合函数求导公式：
$$
   y=f[g(x)]   \\  \frac{∂y}{∂x}=\frac{∂y}{∂u}  ∗ \frac{∂u}{∂x}  \\     y'(x) = f'(u) * g'(x)
$$

2. 正规方程

$$
X = 训练集中的特征值（加上x_0=1） 
$$

$$
y = 训练集中的预测结果
$$

$$
正规方程：θ=(X^T  X)^{−1}  X^T  y
$$

$$
octave写法: pinv(x' * x) * x' * y
$$



梯度下降与正规方程比较

| 梯度下降                                       |                                   正规方程 |
| :--------------------------------------------- | -----------------------------------------: |
| 可能需要做特征变量归一化                       |                     不需要做特征变量归一化 |
| 需要选择学习率α                                |                                没有学习率α |
| 需要多次迭代                                   |                                 不需要迭代 |
| 特征变量极多时也能有效运行，时间复杂度为O(n^2) | 特征变量极多时运行很慢，时间复杂度为O(n^3) |

正规方程的不可逆性（Noninvertibility）

不可逆的可能原因：

1.特征冗余：有不同的特征是线性相关的

2.特征变量过多：样本数量m <= 特征数量n

 

不可逆时，pinv函数仍能使公式正常运行（octave）



### 逻辑回归

分类问题（classification）：解决分类问题的一种方法是使用线性回归，将所有大于0.5的预测映射为1，所有小于0.5的预测映射为0。然而，分类问题的解通常不是一个线性函数。

 

逻辑回归模型：
$$
h_θ(x) = g(θ^T x), 其中
$$

$$
g(z) = \frac{1}{1+e^{-z}}
$$

$$
这个函数又被称为S型函数sigmoid function， 或逻辑函数logistic function
$$

合并上面两个式子得到：
$$
h_θ(x) = \frac{1}{1+e^{-θ^T x}}
$$
上式可解释为
$$
h_θ(x) = P(y=1|x;θ) = 1 - P(y=0|x;θ)
$$

$$
P(y=1|x;θ) + P(y=0|x;θ) = 1
$$



决策边界（decision boundary）：是分隔y = 0 和 y = 1 区域的线，它是由假设函数产生的。
非线性决策边界：Non-linear decision boundaries
$$
h_θ  (x)≥0.5→y=1
$$

$$
h_θ  (x)<0.5→y=0
$$

$$
g(z)≥0.5 \quad when \quad z≥0
$$

记住：
$$
z=0, e^0=1→g(z)=1/2
$$

$$
z→∞, e^(−∞)→0   →    g(z)=1
$$

$$
z→−∞, e^∞→∞   →    g(z)=0
$$

$$
h_θ  (x)=g(θ^T  x)≥0.5 \quad  when \quad θ^T  x≥0
$$

$$
θ^T  x≥0 →  y=1
$$

$$
θ^T  x<0 →  y=0
$$

逻辑回归中的成本函数：
$$
J(θ)= \frac{1}{m} \sum\limits_{i=1}^m  Cost(h_θ  (x^{(i)}), y^{(i)})
$$

$$
Cost(h_θ(x), y)=−log⁡(h_θ(x)) \quad  if \quad y=1
$$

$$
Cost(h_θ(x), y)=−log⁡(1−h_θ(x))   \quad   if \quad y=0
$$

推论：
$$
Cost(h_θ  (x), y)=0 \quad if \quad h_θ  (x)=y
$$

$$
Cost(h_θ  (x), y)→∞ \quad if \quad y=0 \quad and \quad h_θ  (x)→1
$$

$$
Cost(h_θ  (x), y)→∞ \quad if \quad y=1 \quad and \quad h_θ  (x)→0
$$

注：这样定义的损失（成本）函数对于逻辑回归是凸的。

简写损失函数：
$$
Cost(h_θ  (x), y)=−y log⁡(h_θ  (x))  −(1−y)  log⁡(1−h_θ  (x))
$$

$$
J(θ)=-\frac{1}{m} \sum\limits_{i=1}^m  [y^{(i)} log⁡(h_θ(x^{(i)}))+(1−y^{(i)})log⁡(1−h_θ  (x^{(i)})) ]
$$

矢量化的写法：
$$
h=g(X θ)
$$

$$
J(θ)=\frac{1}{m} (−y^T  log⁡(h)  −(1−y)^T  log⁡(1−h) )
$$

梯度下降的迭代式：

Repeat{
$$
θ_j≔θ_j  −\frac{α}{m}  \sum\limits_{i=1}^m  (h_θ(x^{(i)})  −y^{(i)})x_j^{(i)}
$$
}

矢量化的写法：
$$
θ≔θ −\frac{α}{m}  X^T  (g(Xθ)  −y ⃗)
$$
多元分类问题：当拥有两个以上的类别时，将问题划分为n个二元分类问题
$$
y∈\{1, 2, 3, …, n\}
$$

$$
h_θ^{(1)} (x)=P(y=1 |  x ;θ)
$$

$$
h_θ^{(2)} (x)=P(y=2 |  x ;θ)
$$

$$
…
$$

$$
h_θ^{(n)} (x)=P(y=n |  x ; θ)
$$

$$
prediction=\max\limits_{i}⁡( (hθ^{(i)}(x)) )
$$

### 优化算法：

Gradient descent  梯度下降

Conjugate gradient  共轭梯度

BFGS  变尺度法，用BFGS矩阵作为拟牛顿法中的对称正定迭代矩阵的方法

L-BFGS  限制变尺度法

 

后三种算法的优点：

1.不需要手动选择学习率

2.通常收敛速度远远快于梯度下降

缺点：更加复杂



### 过拟合问题

解决过拟合的方法：

1.减少特征量

1）手动选择要保留的特征

2）使用模型选择算法

2.正则化（regularization

保留所有特征，但减少参数的大小

当有大量稍微有用的特征时，正则化工作的很好



改造损失（成本）函数
$$
min⁡(θ \frac{1}{2m} [\sum\limits_{i=1}^m  (h_θ (x^{(i)}) − y^{(i)})^2 + λ\sum\limits_{j=1}^n  θ_j^2 ] )
$$
λ或lambda，是正则化参数（Regularization parameter），它决定了参数膨胀的代价



正则化梯度下降迭代式：

Repeat{
$$
θ_0  := θ_0  −α\frac{1}{m}  \sum\limits_{i=1}^m  (h_θ(x^{(i)}) − y^{(i)})x_0^{(i)}
$$

$$
θ_j  ≔ θ_j  −α[(\frac{1}{m}  \sum\limits_{i=1}^m (h_θ(x^{(i)}) − y^{(i)})x_j^{(i)})+\frac{λ}{m}  θ_j ]
$$

}

或写作:
$$
θ_j≔ θ_j (1−α \frac{λ}{m})  − \frac{α}{m} \sum\limits_{i=1}^m(h_θ(x^{(i)}) − y^{(i)})  x_j^{(i)}
$$
正则化正规方程
$$
θ=(X^T X+λL)^{−1}  X^T y
$$

$$
定义 L = \left[\begin{array}{lcr}0& & & & \\ &1& & & \\ & &1& & \\ & & &...& \\ & & & &1\end{array}\right]
$$



正则化逻辑回归

正则化逻辑回归损失函数
$$
J(θ)=−\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log⁡(h_θ(x^{(i)}))+(1−y^{(i)})log⁡(1−h_θ (x^{(i)}))]+\frac{λ}{2m}\sum\limits_{j=1}^n θ_j^2
$$
正则化逻辑回归梯度下降迭代式

Repeat{
$$
θ_0≔ θ_0  −\frac{α}{m}\sum\limits_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x_0^{(i)})
$$

$$
	θ_j≔ θ_j −α[\frac{1}{m}\sum\limits_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j ]   \quad j∈\{1, 2, 3, …, n\}
$$

}



### 神经网络：（Neural Networks）

输入层（input layer）-> 隐藏层（hidden layer）-> 输出层（output layer）

 

隐藏层节点被称为“激活单位”（activation unit）
$$
a_i^{(j)}: 第 j 层的第 i 个激活单元\\
Θ^{(j)}:第j层到第j+1层的权重控制函数矩阵
$$
当只有一层隐藏层时，神经网络如下图所示：
$$
\left[\begin{array}{lcr}x_0 \\ x_1 \\ x_2 \\ x_4\end{array}\right]→\left[\begin{array}{lcr}a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)}\end{array}\right]→h_θ (x)
$$
每个激活节点的值定义如下：
$$
a_1^{(2)}=g(Θ_{10}^{(1)}x_0 + Θ_{11}^{(1)}x_1 + Θ_{12}^{(1)}x_2 + Θ_{13}^{(1)}x_3)
$$

$$
a_2^{(2)}=g(Θ_{20}^{(1)}x_0 + Θ_{21}^{(1)}x_1 + Θ_{22}^{(1)}x_2 + Θ_{23}^{(1)}x_3)
$$

$$
a_3^{(2)}=g(Θ_{30}^{(1)}x_0 + Θ_{31}^{(1)}x_1 + Θ_{32}^{(1)}x_2 + Θ_{33}^{(1)}x_3)
$$

$$
h_θ(x)=a_1^{(3)}=g(Θ_{10}^{(2)}a_0^{(2)} + Θ_{11}^{(2)}a_1^{(2)} + Θ_{12}^{(2)}a_2^{(2)} + Θ_{13}^{(2)}a_3^{(2)})
$$

$$
每一层都有自己的权重矩阵Θ^{(j)}
$$

$$
权重矩阵的维度定义：如果 j 层有 S_j  个单元，j + 1层有 S_{j+1}  个单元，Θ^{(j)}的维度为 S_{j+1}∗(S_j+1)
$$

$$
偏差节点 x_0=a_0^{(j)}=1 (bias nodes)  在输入节点中写出，不在输出节点中写出。
$$



向量化神经网络的表示：

新定义一个变量z，包含g函数中的所有参数：
$$
a_1^{(2)}=g(z_1^{(2)})
$$

$$
a_2^{(2)}=g(z_2^{(2)})
$$

$$
a_3^{(2)}=g(z_3^{(2)})
$$

对于在第 j 层的第 k 个节点，变量z为：
$$
z_k^{(j)}= Θ_{k, 0}^{(j−1)}x_0 + Θ_{k, 1}^{(j−1)}x_1 + … + Θ_{k, n}^{(j−1)}x_n
$$

$$
向量化表示x和z^{(j)}
$$

$$
x=\left[\begin{array}{lcr}x_0 \\ x_1 \\ … \\ x_n\end{array}\right] \quad z^{(j)}=\left[\begin{array}{lcr}z_1^{(j)} \\ z_2^{(j)} \\ … \\ z_n^{(j)}\end{array}\right]
$$

$$
设 x=a^{(1)} ，则：
$$

$$
z^{(j)}= Θ^{(j−1)}a^{(j−1)}
$$

$$
a^{(j)}=g(z^{(j)})
$$

注意： 在神经网络的最后一层，我们所做的与逻辑回归中所做的完全相同。
$$
例子：AND, NOR 和 OR 的 Θ^{(1)}  矩阵为
$$

$$
AND: Θ^{(1)}=[−30 \quad 20 \quad 20]
$$

$$
NOR: Θ^{(1)}=[10 \quad −20 \quad −20]
$$

$$
OR: Θ^{(1)}=[−10 \quad 20 \quad 20]
$$

组合它们得到XNOR的效果：
$$
\left[\begin{array}{lcr}x_0 \\ x_1 \\ x_2\end{array}\right]→\left[\begin{array}{lcr}a_1^{(2)} \\ a_2^{(2)}\end{array}\right]→\left[\begin{array}{lcr}a^{(3)}\end{array}\right]→h_θ (x)
$$

$$
Θ^{(1)}=\left[\begin{array}{lcr}−30&20&20 \\ 10&−20&−20\end{array}\right] \quad Θ^{(2)}=\left[\begin{array}{lcr}−10&20&20\end{array}\right]
$$

$$
a^{(2)}=g(Θ^{(1)}∗x) \quad a^{(3)}=g(Θ^{(2)}∗a^{(2)}) \quad h_Θ(x)=a^{(3)}
$$



神经网络反向传播算法
$$
设置Δ_{ij}^{(l)} = 0 \quad (对于所有l,i,j)
$$
For i = 1 to m
$$
设置 \quad a^{(1)} = x^{(i)}
$$

$$
执行正向传播来计算a^{(l)}(对于l=2,3,...,L)
$$

$$
使用y^{(i)},计算δ^{(L)} = a^{(L)} - y^{(i)}
$$

$$
计算δ^{(L-1)},δ^{(L-2)},...,δ^{(2)}
$$

$$
Δ_{ij}^{l}:=Δ_{ij}^{l} + a_j^{(l)}δ_i^{(l+1)}
$$

------

$$
D_{ij}^{(l)} := \frac{1}{m}Δ_{ij}^{(l)} \quad if \quad j = 0
$$

$$
D_{ij}^{(l)} := \frac{1}{m}Δ_{ij}^{(l)} + λΘ_{ij}^{(l)} \quad if \quad j ≠ 0
$$

$$
\frac{∂}{∂Θ_{ij}^{(l)}} J(Θ) = D_{ij}^{(l)}
$$





使用神经网络之前请注意进行梯度检查和随机初始化



在模型训练的过程中，可通过画出学习曲线判断模型处于高偏差（bias）还是高方差（variance），根据不同的情况调整参数。



### 支持向量机

个人理解为将逻辑回归的一种优化方法

逻辑回归损失函数：
$$
J(θ)=−\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log⁡(h_θ(x^{(i)}))+(1−y^{(i)})log⁡(1−h_θ (x^{(i)}))]+\frac{λ}{2m}\sum\limits_{j=1}^n θ_j^2
$$
支持向量机损失函数：
$$
J(θ)=C\sum\limits_{i=1}^m[y^{(i)}cost_1⁡(\theta^Tx^{(i)})+(1−y^{(i)})cost_0⁡(\theta^Tx^{(i)})]+\frac{1}{2}\sum\limits_{j=1}^n θ_j^2
$$
支持向量机通过调整C的大小来应对拟合不足/过拟合的情况



核函数：通过某非线性变换 φ( x) ，将输入空间映射到高维特征空间。



使用核函数的支持向量机主要用于特征数量只有样本数量的1%左右的情况（神经网络能应对大部分情况，但训练速度可能比支持向量机慢）

## 非监督学习

与监督学习不同，非监督学习使用的数据集是没有标注的数据集。

非监督学习在机器学习课程中主要介绍了两种应用：聚类与降维

### 聚类

将未标注的数据集根据特征值的相似程度自行分为几类

常用的算法是K均值算法

#### K均值算法

随机产生k个聚类中心，之后重复以下两个步骤

1.根据和每个聚类中心的距离，将所有样本标记为距离最近的聚类中心的类型

2.由每个类的所有样本计算出该类现在的中心点，将聚类中心移动到它代表的类的中心点

直到聚类中心移动的距离少于预定值



类的数量K的选择：画出损失函数后根据手臂法则选择（选择拐点或附近的K值）

### 降维

将高维度数据降低到较低的维度，主要使用主成分分析（PCA）算法

#### 主成分分析

将n维数据降维至k维（k<=n)

1.数据预处理：特征缩放，均值归一化

2.计算协方差矩阵：
$$
\Sigma = \frac{1}{m}\sum\limits_{i=1}^n (x^{(i)}) (x^{(i)})^T
$$
3.计算协方差矩阵的本征矢量（eigenvectors）
$$
[U, S, V] = svd(Sigma);
$$
4.设置Ureduce = U(:, 1 : k);

5.计算z = Ureduce' * x;

z既是降维后的数据集



评估差异值损失的公式:
$$
\frac{\frac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-x^{(i)}_approx)^2}{\frac{1}{m}\sum\limits_{i=1}^m(x^{(i)})^2}
$$
![image](https://user-images.githubusercontent.com/9285301/62423341-51123780-b6f2-11e9-819f-95c5709dec9b.png)

或者计算差异值保留比例的公式：
$$
\frac{\sum\limits_{i=1}^k S_{ii}}{\sum\limits_{i=1}^m S_{ii}}
$$
降维的作用：

1.压缩内存/硬盘空间

2.提高训练速度

3.可视化（降维至2维或3维)

注意：不要使用降维来应对过拟合



## 异常检测

### 多元高斯（正态）分布

设有训练集X
$$
μ = \frac{1}{m}\sum\limits_{i=1}^m x^{(i)} \quad Σ = \frac{1}{m} \sum\limits_{i=1}^m (x^{(i)}-μ)(x^{(i)}-μ)^T
$$

$$
p(x; μ; Σ) = \frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-μ)^T Σ^{-1} (x-μ))
$$

## 推荐系统

### 协同过滤算法

$$
J(X,\theta) = \frac{1}{2} \sum\limits_{(i,j):r(i,j) = 1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{λ}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^n(x_k^{(i)})^2 + \frac{λ}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{k=1}^n(\theta_k^{(j)})^2
$$

注意，实际应用中协同过滤也需要均值归一化



## 大数据处理

1.将批量梯度下降算法换为随机批量下降或者小批量梯度下降

2.分布式处理：建议查阅google三驾马车论文：Google-Bigtable、Google-File-System、Google-MapReduce。



## 项目：图像OCR

主要讲述了机器学习系统流水线，数据增广，以及流水线的上限分析以确定优化重点





## 总结

经过近两个月的学习，大体掌握了机器学习的常用算法，公式，理解了神经网络的工作原理，但该课程发布时间略早，现有主流框架已有大量更新，本教程的神经网络只涉及NN,也就是现有主流网络的最后3层全连接层。为了更好的掌握现在（2019年下半年）主流网络的特点，还需不断学习，与各位同学的交流让我获益良多，希望热爱学习的热情永不熄灭，书山有路勤为径，学海无涯苦作舟。
